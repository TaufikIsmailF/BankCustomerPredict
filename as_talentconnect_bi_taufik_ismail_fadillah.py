# -*- coding: utf-8 -*-
"""AS-TalentConnect BI-Taufik Ismail Fadillah.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bpkZe-egpNejNx7t0mRAoXYDU8mSM-Jq
"""

import pandas as pd
pd.set_option("display.max_columns",None)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb

"""**Nama : Taufik Ismail Fadillah**

# Objective

Objective machine learning ini adalah untuk **memprediksi Bank Customer untuk Churn berdasarkan fitur yang ada dalam dataset**.
"""

# Data fitur dan deskripsi
features = {
    "Feature": [
        "customer_id", "credit_score", "country", "gender", "age",
        "tenure", "balance", "products_number", "credit_card",
        "active_member", "estimated_salary", "churn"
    ],
    "Description": [
        "Unique identifier for each customer",
        "Credit score of the customer",
        "Customer’s country of residence",
        "Customer’s gender",
        "Customer’s age",
        "Number of years the customer has been with the bank",
        "Account balance of the customer",
        "Number of products used by the customer",
        "Whether the customer has a credit card (1: Yes, 0: No)",
        "Whether the customer is an active member (1: Yes, 0: No)",
        "Estimated annual salary of the customer",
        "Target variable indicating if the customer churned (1: Yes, 0: No)"
    ]
}

# Membuat DataFrame
feature_table = pd.DataFrame(features)

"""# Detail info dataset"""

feature_table

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=feature_table)



"""# Data Preprocessing"""

cr = pd.read_csv('/content/Bank Customer Churn Prediction.csv')
cr

cr.info()

cr.tail()

"""# Missing Value Handling"""

# sesuai deskripsinya, banyak yang dianggap NaN karena memang ada value yang berisikan NA atau None
# selanjutnya kita coba load dengan cara replace NaN

# missing value check
cr.isna().sum()*100/len(cr)



# Cek jumlah missing values di setiap kolom
print(cr.isnull().sum())

# Cek data duplikat
print(f"Jumlah duplikat: {cr.duplicated().sum()}")

"""# Feature Engineering"""

# Daftar kolom yang akan diubah menjadi float
float_columns = ['tenure']

# Mengonversi kolom menjadi float
cr[float_columns] = cr[float_columns].replace({',': '', ' ': ''}, regex=True)  # Menghapus koma atau spasi jika ada
cr[float_columns] = cr[float_columns].astype(float)

# Verifikasi hasil
print(cr[float_columns].info())

categorical = ['gender','age','products_number','credit_card','active_member','churn']

numerical = ['balance','estimated_salary','tenure']

cr['country'].value_counts()

cr['country'].value_counts()

"""country 0 : France
country 1 : Germany
country 2 : Spain
"""

dummies_country = pd.get_dummies(cr['country'],prefix='et')

# tujuannya untuk menghindai multicolinearity
dummies_country.head()

cr_encoded = pd.concat([cr, dummies_country], axis=1)
cr_encoded = cr_encoded.drop('country', axis=1)
cr_encoded["et_France"] = cr_encoded["et_France"].astype(int)
cr_encoded["et_Germany"] = cr_encoded["et_Germany"].astype(int)
cr_encoded["et_Spain"] = cr_encoded["et_Spain"].astype(int)
cr_encoded.head()



from sklearn.preprocessing import LabelEncoder

# Mengubah variabel kategorikal menjadi numerik menggunakan Label Encoding
label_encoder = LabelEncoder()
for column in cr.columns:
    if cr[column].dtype == 'object':
        cr[column] = label_encoder.fit_transform(cr[column])

# Memeriksa data setelah encoding
cr.head()

cr['gender'].value_counts()

cr['gender'].value_counts()

cr['tenure'].value_counts()

print(cr['balance'].value_counts())

"""# Standard EDA"""

cr.info()

cr.describe()

"""mean,median, max, reasonable.. tidak ada yang 'tidak masuk akal'

### **Analisa mengenai distribusi dari 'churn'**
"""

print(cr['active_member'].value_counts(), '\n')

print(cr['churn'].value_counts(), '\n')

sns.countplot(data=cr, x='churn')

"""#### **Interpretasi**

Jika dilihat dari data 'churn', maka dapat disimpulkan bahwa target merupakan imbalance, maka akan berpengaruh kepada teknis dalam permodelan, untuk dianalisis lebih lanjut menggunakan ROC-AUC untuk menghindari bias interpretasi jika hanya menggunakan confusion metrix sebagau evaluationnya

customers yang tidak churn dengan jumlah **7963**, sedangkan yang churn **2037**

### **Outlier handling disetiap fitur**
"""

plt.figure(figsize=(12,6))

features = numerical
for i in range(0, len(features)):
    plt.subplot(1, len(features), i+1)
    sns.boxplot(y=cr[features[i]], color='red')
    plt.tight_layout()

"""#### **Interpretasi**

tidak terlihat adanya outlier jika dilihat melalui boxplot

### **Visualisasi distribusi column 'numerical'**
"""

plt.figure(figsize=(12,6))

features = numerical
for i in range(0, len(features)):
    plt.subplot(2, len(features)//2 + 1, i+1)
    sns.kdeplot(x=cr[features[i]], color='skyblue')
    plt.xlabel(features[i])
    plt.tight_layout()

"""#### **Interpretasi**

**Jenis Distribusi**

**1. Balance:**

Distribusi tampak bimodal dengan dua puncak. Satu puncak besar di sekitar 0 (mungkin banyak pelanggan dengan saldo nol) dan satu puncak lainnya di sekitar saldo yang lebih tinggi (sekitar 100.000 - 150.000).
Hal ini menunjukkan adanya dua kelompok pelanggan: pelanggan dengan saldo kecil atau nol, dan pelanggan dengan saldo besar.

**2. Estimated Salary:**

Distribusi cukup seragam (uniform), dengan persebaran data yang hampir merata di seluruh rentang dari 0 hingga 200.000.
Tidak ada puncak signifikan, menunjukkan bahwa gaji rata-rata pelanggan tidak memiliki pola khusus.

**Tenure:**

Distribusi tampak merata dengan sedikit fluktuasi. Ada puncak kecil di beberapa titik tertentu, menunjukkan pelanggan cenderung bertahan pada periode tertentu (misalnya, 1, 5, atau 10 tahun).
Secara umum, persebarannya hampir seragam di rentang 0 hingga 10 tahun.

#### **Insight**

**1. Peluang Churn Berdasarkan Saldo:**

Pelanggan dengan saldo nol tampaknya memiliki risiko churn lebih tinggi dibandingkan dengan pelanggan dengan saldo yang signifikan. Ini dapat mengindikasikan bahwa pelanggan yang tidak aktif secara finansial di bank cenderung churn.
Pelanggan dengan saldo besar mungkin lebih loyal, tetapi mereka juga memerlukan perhatian khusus untuk mempertahankan mereka.

**2. Pengaruh Gaji terhadap Churn:**

Gaji tampaknya tidak menjadi faktor signifikan untuk memprediksi churn karena distribusi datanya cukup merata tanpa konsentrasi khusus.

**3. Tenure dan Retensi Pelanggan:**

Fluktuasi tenure menunjukkan bahwa pelanggan cenderung churn pada tahun-tahun tertentu. Periode tertentu mungkin menunjukkan titik kritis di mana bank perlu meningkatkan upaya retensi, seperti di tahun pertama atau tahun ke-10.

### **Analisa heatmap untuk column 'numerical'**
"""

plt.figure(figsize=(8,6))
correlation = cr[numerical].corr()
sns.heatmap(correlation, annot=True, fmt='.2f')

"""#### **Interpretasi**

Dapat diasumsikan bahwa threshold secara signifikan = 0,8

semakin lamanya tenure customer semakin tinggi pula balance mereka **'tabungan'**

**1. Balance vs Estimated Salary (Koefisien Korelasi: 0.01):**

Korelasi sangat rendah (hampir nol), menunjukkan tidak ada hubungan linier yang signifikan antara saldo rekening dan estimasi gaji pelanggan.
Hal ini mengindikasikan bahwa tingkat saldo rekening pelanggan tidak berkaitan dengan estimasi gaji mereka.

**2. Balance vs Tenure (Koefisien Korelasi: -0.01):**

Korelasi negatif yang sangat rendah (hampir nol), yang berarti tidak ada hubungan signifikan antara saldo rekening dan lama waktu pelanggan menjadi anggota bank.
Saldo pelanggan tampaknya tidak dipengaruhi oleh seberapa lama mereka menjadi anggota.

**3. Estimated Salary vs Tenure (Koefisien Korelasi: 0.01):**

Korelasi sangat rendah, menunjukkan tidak ada hubungan signifikan antara estimasi gaji dan lama waktu menjadi anggota.
Gaji pelanggan tidak memiliki pola tertentu terkait seberapa lama mereka menjadi pelanggan bank.

#### **Insight**

**Prediksi Churn Berdasarkan Korelasi:**

Dari heatmap, korelasi antar variabel numerik ini sangat rendah, menunjukkan bahwa fitur-fitur balance, estimated_salary, dan tenure secara individu tidak memiliki hubungan kuat satu sama lain.
Namun, korelasi rendah tidak berarti bahwa variabel ini tidak relevan untuk memprediksi churn. Analisis lebih lanjut dengan model prediktif seperti regresi logistik atau Random Forest dapat membantu mengidentifikasi dampaknya secara lebih mendalam.

**Penanganan Data untuk Model:**

Fitur ini dapat tetap digunakan untuk model prediktif karena korelasi antar fitur rendah, yang berarti tidak ada risiko multikolinearitas.
Hubungan non-linier atau interaksi antar variabel mungkin lebih signifikan dalam memprediksi churn.
Analisis yang Disarankan:

**Lakukan analisis tambahan seperti:**

**Boxplot churn vs balance:** Untuk melihat apakah pelanggan yang churn memiliki saldo yang berbeda dibandingkan yang tidak churn.

**Boxplot churn vs tenure:** Untuk memahami apakah pelanggan dengan masa keanggotaan tertentu lebih cenderung churn.
Modeling: Menggunakan machine learning untuk menentukan fitur mana yang memiliki pengaruh terbesar terhadap churn.

### **Analisa korelasi distribusi 'churn' pada setiap kolom**
"""

# Memilih kolom bertipe float
float_cols = cr.select_dtypes(include=['float']).columns

# Memastikan kolom 'churn' ada di dalam data untuk hue
if 'churn' not in float_cols:
    float_cols = float_cols.append(pd.Index(['churn']))

# Subset data dengan kolom bertipe float
float_data = cr[float_cols]

# Pairplot dengan hue 'churn'
sns.pairplot(data=float_data, hue='churn')

"""#### **Interpretasi**

1. **Tenure:** jika tenure customer kurang dari <4 tahun maka kemungkinan besar customer akan churn, sedangkan yang tenure >5 tahun, customer tidak akan churn

2. **Balance:** jika balance customer tinggi, ada kemungkinan ia akan churn, namun distribusi tidak dominan terhadap customer yang memiliki balance tinggi dan ia tidak churn

3. **estimated_salary:** interpretasi yang sama dengan balance, ada kemungkinan customer churn jika estimated salary tinggi, namun data menunjukan, sekitar dibawah 50.000 salary, customers akan churn

# **Train-Test Split**
"""

from sklearn.model_selection import train_test_split

# drop certain columns
cr = cr.drop(['customer_id'], axis = 1)

# cretate predictor variables as X
X = cr.drop(['churn'], axis = 1)
# create target data as y
y = cr['churn']

# split data to train and test data
# where 30% of churn_data are test data
# stratify data based on y varibales
# and freeze RNG with random_state = 1000
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.3,
    stratify = y,
    random_state=1000
)

# check train data dimension
X_train.shape

# check test data dimension
X_test.shape

# check train target counts
y_train.value_counts()

# check test target counts
y_test.value_counts()

"""# **Deep Dive EDA**"""

cr.head()

"""### **Pertanyaan?**"""

# 1. Bagaimana rata-rata balance customer churn dan tidak churn?
# 2. Siapa top 10 customer berdasarkan balance, customer churn atau tidak churn?
# 3. Bagaimana rata2 total charges untuk tiap credit_card dibandingkan active_member?
# 4. Apakah rata-rata balance atau estimated_salary lebih tinggi untuk pelanggan yang churn?
# 5. Apakah ada perbedaan signifikan antara pelanggan yang churn dan yang tidak churn dalam hal tenure atau age?
# 6. Apakah pelanggan yang lebih muda lebih cenderung churn dibandingkan yang lebih tua?
# 7. Apa distribusi masing-masing kategori pada fitur kategorikal seperti country dan gender? (Gunakan countplot)
# 8. Apakah ada perbedaan tingkat churn berdasarkan country atau gender? (Gunakan countplot atau barplot)
# 9. Apakah ada hubungan antara fitur credit_card atau active_member dengan churn? (Gunakan crosstab atau groupby)
# 10. Apakah kelompok pelanggan dengan balance tinggi dan credit_score rendah lebih cenderung churn?
# 11. Apa fitur yang paling relevan untuk memprediksi churn? Lakukan seleksi fitur menggunakan teknik seperti feature importance atau correlation thresholding

"""#### **1. Bagaimana Korelasi matriks?**"""

# create correlation matrix
X_train.corr().style.background_gradient(cmap='coolwarm')

# pull numerical column
var_name = X_train.select_dtypes(include = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']).columns

var_name

"""#### **2. Pembuatan Subplot 4 x 2 untuk melihat distribusi masing-masing feature terhadap target 'churn'**"""

# # create subplot 4 x 2
fig, axes = plt.subplots(4,2, sharex=False, sharey = False, figsize=(10,15), constrained_layout = True)

fig.suptitle('Numerical Predictors vs Target Variable')

col_index = 0

for row in range(4):
  for col in range(2):
    sns.boxplot(ax = axes[row,col], x=y_train, y = X_train[var_name[col_index]])
    axes[row,col].set_title(var_name[col_index])
    # axes[row,col].xticks(color='w')
    col_index += 1

"""#### **3. Distribusi Usia Customer?**"""

# Distribusi Usia Nasabah dan Usia dengan Kemungkinan Churn Terbesar!

# Distribusi usia nasabah
plt.figure(figsize=(10, 6))
sns.histplot(cr['age'], bins=30, kde=True)
plt.title('Distribusi Usia Nasabah')
plt.xlabel('Usia')
plt.ylabel('Frekuensi')
plt.show()

"""#### **4. Bagaimana distribusi customer untuk churn?**"""

# Usia dengan kemungkinan churn terbesar
plt.figure(figsize=(10, 6))
sns.boxplot(x='churn', y='age', data=cr)
plt.title('Distribusi Usia berdasarkan Churn')
plt.xlabel('Churn')
plt.ylabel('Usia')
plt.show()

"""##### **Interpretasi**

Rata-rata usia yang akan churn, sekitar 45-50 tahun, namun terdapat outlier yang unik, sekitar usia 70-84 tahun, customer akan churn.

#### **5. Apakah ada hubungan antara fitur credit_card atau active_member dengan churn?** (Gunakan crosstab atau groupby)
"""

# Korelasi antara fitur credit_card dan kategori active_member dengan 'churn'
credit_corr = pd.crosstab(cr['credit_card'], cr['active_member'])
print(credit_corr)

# Visualisasi korelasi terhadap churn
plt.figure(figsize=(10, 6))
sns.countplot(x='credit_card', hue='churn', data=cr)
plt.title('Korelasi credit_card dan churn')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='active_member', hue='churn', data=cr)
plt.title('Korelasi fitur active_member dan churn')
plt.show()

"""#### **6. Bagaimana rata2 balance untuk tiap credit_card dibandingkan active_member?**

"""

#Bagaimana rata2 total charges untuk tiap credit_card dibandingkan dengan active_member
avgspendcredit = cr.groupby(['credit_card','churn']).agg(avg_spend=('balance','mean')).reset_index()
avgspendcredit

cr = pd.read_csv('/content/Bank Customer Churn Prediction.csv')
cr

"""#### **7. Siapa top 10 customer berdasarkan balance, customer churn atau tidak churn?**"""

#Siapa sajakah top 10 customer berdasarkan balance dan apakah ada yang churn?
top10 = cr.groupby(['customer_id','churn']).agg(total_spend=('balance','sum')).reset_index().sort_values('total_spend',ascending=False).head(10)
top10

"""#### **8. Apakah rata-rata balance atau estimated_salary lebih tinggi untuk pelanggan yang churn?**"""

# Menghitung rata-rata balance untuk pelanggan yang churn dan tidak churn
average_balance = cr.groupby('churn')['balance'].mean()

# Menghitung rata-rata estimated_salary untuk pelanggan yang churn dan tidak churn
average_salary = cr.groupby('churn')['estimated_salary'].mean()

# Menampilkan hasil
print("Rata-rata Balance:")
print(average_balance)

print("\nRata-rata Estimated Salary:")
print(average_salary)

"""#### **9. Bagaimana persentase churn berdasarkan gender?**"""

# Menghitung persentase churn berdasarkan gender
gender_churn = pd.crosstab(cr['gender'], cr['churn'], normalize='index')
print(gender_churn)

# Visualisasi perbedaan gender dan churn
plt.figure(figsize=(10, 6))
sns.countplot(x='gender', hue='churn', data=cr)
plt.title('Perbedaan Gender terhadap Kemungkinan Churn')
plt.show()

"""#### **10. Apakah ada perbedaan signifikan antara pelanggan yang churn dan yang tidak churn dalam hal tenure atau age?**"""

from scipy.stats import ttest_ind, mannwhitneyu

# Eksplorasi: Rata-rata tenure dan age berdasarkan churn
average_tenure = cr.groupby('churn')['tenure'].mean()
average_age = cr.groupby('churn')['age'].mean()

print("Rata-rata Tenure:")
print(average_tenure)

print("\nRata-rata Age:")
print(average_age)

# Uji Statistik: t-test untuk tenure
tenure_churn = cr[cr['churn'] == 1]['tenure']
tenure_no_churn = cr[cr['churn'] == 0]['tenure']

t_stat_tenure, p_value_tenure = ttest_ind(tenure_churn, tenure_no_churn)
print("\nT-test untuk Tenure:")
print(f"T-Stat: {t_stat_tenure}, P-Value: {p_value_tenure}")

# Uji Statistik: t-test untuk age
age_churn = cr[cr['churn'] == 1]['age']
age_no_churn = cr[cr['churn'] == 0]['age']

t_stat_age, p_value_age = ttest_ind(age_churn, age_no_churn)
print("\nT-test untuk Age:")
print(f"T-Stat: {t_stat_age}, P-Value: {p_value_age}")

# Alternatif: Mann-Whitney U Test jika data tidak normal
u_stat_tenure, p_u_tenure = mannwhitneyu(tenure_churn, tenure_no_churn)
u_stat_age, p_u_age = mannwhitneyu(age_churn, age_no_churn)

print("\nMann-Whitney U Test untuk Tenure:")
print(f"U-Stat: {u_stat_tenure}, P-Value: {p_u_tenure}")

print("\nMann-Whitney U Test untuk Age:")
print(f"U-Stat: {u_stat_age}, P-Value: {p_u_age}")

"""#### **Interpretasi dan insight**

**1. Tenure (Masa Keanggotaan):**

**Rata-rata:**
Pelanggan yang tidak churn: 5.03 tahun.
Pelanggan yang churn: 4.93 tahun.
Perbedaan kecil (sekitar 0.1 tahun).

**Uji Statistik:**
`
**t-test:** P-Value = 0.16 (tidak signifikan, > 0.05).

**Mann-Whitney U:** P-Value = 0.16 (tidak signifikan, > 0.05).

**Kesimpulan:** Tidak ada perbedaan signifikan dalam tenure antara pelanggan yang churn dan yang tidak churn. Masa keanggotaan tampaknya tidak menjadi faktor utama dalam menentukan churn.

**2. Age (Usia):**

**Rata-rata:**
Pelanggan yang tidak churn: 37.41 tahun.
Pelanggan yang churn: 44.84 tahun.
Pelanggan yang churn rata-rata 7.43 tahun lebih tua.

**Uji Statistik:**

**t-test:** P-Value ≈ 0 (sangat signifikan, < 0.05).

**Mann-Whitney U:** P-Value ≈ 0 (sangat signifikan, < 0.05).

**Kesimpulan:** Ada perbedaan signifikan dalam usia antara pelanggan yang churn dan yang tidak churn. Pelanggan yang churn cenderung lebih tua.

**Rekomendasi Bisnis untuk Mengurangi Churn**

**1. Segmentasi dan Strategi Berdasarkan Usia:**

Pelanggan yang lebih tua (usia > 40) memiliki risiko churn lebih tinggi.

*   Strategi
    Tawarkan paket atau layanan yang lebih relevan dengan kebutuhan pelanggan yang lebih tua, seperti personalisasi layanan atau penawaran berbasis pengalaman.
    Edukasi pelanggan yang lebih tua tentang manfaat
*   Strategi
    Edukasi pelanggan yang lebih tua tentang manfaat layanan perusahaan, karena mereka mungkin merasa kurang nyaman dengan teknologi baru.




**2. Fokus pada Kualitas Layanan, Bukan Durasi Keanggotaan:**

*   Karena tenure tidak signifikan dalam memengaruhi churn, fokuslah pada kualitas pengalaman pelanggan, bukan hanya mempertahankan mereka lebih lama.
*   Strategi:
Tingkatkan engagement dengan program loyalitas atau penghargaan berdasarkan aktivitas, bukan hanya lama keanggotaan.
Berikan survei kepuasan pelanggan secara berkala untuk memahami kebutuhan mereka lebih baik.

**3. Peningkatan Customer Support untuk Kelompok Berisiko:**

Pelanggan yang lebih tua mungkin memerlukan lebih banyak dukungan teknis atau bantuan langsung.
*   Strategi:
*   Buat tim khusus untuk menangani segmen pelanggan yang lebih tua.
*   Sediakan tutorial sederhana, layanan konsultasi langsung, atau panduan berbasis video untuk memudahkan mereka.

**4. Personalized Retention Offers:**

*  Gunakan analisis churn untuk menawarkan diskon atau penawaran khusus kepada pelanggan yang lebih tua sebelum mereka churn.
*  Contoh: Jika pelanggan yang lebih tua cenderung churn setelah 4-5 tahun, kirimkan promosi khusus saat mereka mendekati titik waktu ini.

**5. Program Edukasi dan Training Digital:**

*  Jika churn disebabkan oleh ketidaknyamanan pelanggan yang lebih tua dengan teknologi, sediakan program pelatihan yang membantu mereka lebih mudah menggunakan layanan perusahaan.

#### **11. Apakah ada perbedaan signifikan pengaruh usia dengan faktor lain seperti tenure, balance, atau estimated_salary untuk wawasan lebih mendalam terhadap churn?**
"""

# Membuat kategori usia
cr['age_group'] = pd.cut(cr['age'], bins=[0, 40, 50, 100], labels=['<40', '40-50', '>50'])

# Eksplorasi rata-rata faktor lain berdasarkan churn dan kelompok usia
average_factors = cr.groupby(['age_group', 'churn'])[['tenure', 'balance', 'estimated_salary']].mean()
print("Rata-rata Faktor Lain Berdasarkan Kelompok Usia dan Churn:")
print(average_factors)

# Visualisasi interaksi dengan boxplot
import seaborn as sns
import matplotlib.pyplot as plt

# Balance vs Churn per kelompok usia
plt.figure(figsize=(10, 6))
sns.boxplot(x='churn', y='balance', hue='age_group', data=cr)
plt.title('Distribusi Balance Berdasarkan Churn dan Kelompok Usia')
plt.show()

# Tenure vs Churn per kelompok usia
plt.figure(figsize=(10, 6))
sns.boxplot(x='churn', y='tenure', hue='age_group', data=cr)
plt.title('Distribusi Tenure Berdasarkan Churn dan Kelompok Usia')
plt.show()

import statsmodels.api as sm
from statsmodels.formula.api import ols

# Two-Way ANOVA untuk usia dan balance terhadap churn
model = ols('churn ~ C(age_group) * balance', data=cr).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print("Two-Way ANOVA untuk Usia dan Balance terhadap Churn:")
print(anova_table)

# Two-Way ANOVA untuk usia dan tenure terhadap churn
model2 = ols('churn ~ C(age_group) * tenure', data=cr).fit()
anova_table2 = sm.stats.anova_lm(model2, typ=2)
print("\nTwo-Way ANOVA untuk Usia dan Tenure terhadap Churn:")
print(anova_table2)

"""##### **Interpretasi dan insight**

**Kesimpulan dan Rekomendasi:**

*   **Usia Berpengaruh Signifikan Terhadap Churn:**
    
    Baik dalam uji age_group terhadap balance maupun tenure, kelompok usia memiliki pengaruh signifikan terhadap churn. Strategi retensi pelanggan dapat disesuaikan dengan kelompok usia tertentu untuk mengurangi churn.

*   **Balance Berpengaruh Signifikan terhadap Churn:**

    Balance memiliki pengaruh signifikan terhadap churn, dan ada interaksi signifikan dengan kelompok usia. Ini berarti penting untuk memahami bagaimana saldo akun mempengaruhi perilaku churn untuk setiap kelompok usia. Misalnya, untuk pelanggan yang lebih tua, saldo yang lebih tinggi mungkin mengurangi kemungkinan churn.

*   **Tenure Tidak Signifikan:**

    Tenure (durasi langganan) tidak memiliki pengaruh signifikan terhadap churn, dan interaksinya dengan usia juga tidak signifikan. Ini menunjukkan bahwa lama berlangganan tidak cukup untuk memprediksi apakah pelanggan akan churn atau tidak.

**Rekomendasi Bisnis:**

*   **Segmentasi Usia dan Balance:**

    Pertimbangkan untuk menargetkan program retensi atau penawaran khusus berdasarkan kelompok usia dan saldo akun. Pelanggan yang lebih tua dan dengan saldo lebih tinggi mungkin membutuhkan jenis penawaran atau komunikasi yang berbeda.

*   **Fokus pada Pengelolaan Saldo untuk Mengurangi Churn:**

    Jika analisis lebih lanjut menunjukkan bahwa pelanggan dengan saldo lebih tinggi lebih cenderung tetap bertahan, buat insentif untuk mendorong pelanggan memiliki saldo yang lebih tinggi (misalnya, memberikan reward atau layanan tambahan bagi pelanggan dengan saldo besar).

*  **Tidak Perlu Fokus pada Tenure:**

    Mengingat tenure tidak signifikan, bisnis mungkin perlu fokus pada faktor lain seperti interaksi dengan produk atau layanan, bukan hanya lama pelanggan terdaftar.

# Modeling Machine Learning without SMOTE
"""

import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns

"""## Pipeline for initialize models"""

# Pipeline untuk DecisionTreeClassifier
pipeline_dtc = Pipeline([
    ('model', DecisionTreeClassifier(random_state=42))
])

# Pipeline untuk XGBoost Classifier
pipeline_xgbc = Pipeline([
    ('model', XGBClassifier(random_state=42))
])

# Pipeline untuk LightGBM Classifier
pipeline_lgbmc = Pipeline([
    ('model', LGBMClassifier(random_state=42))
])

# Pipeline untuk HistGradientBoostingClassifier
pipeline_hgbc = Pipeline([
    ('model', HistGradientBoostingClassifier(max_iter=100, learning_rate=0.1, max_depth=3))
])

# Pipeline untuk RandomForsetClassifier
pipeline_rf = Pipeline([
    ('model', RandomForestClassifier())
])

"""## Hyperparameter grids"""

# Parameter grid untuk DecisionTreeClassifier
param_grid_dtc = {
    'model__max_depth': [None, 10, 20, 30],
    'model__min_samples_split': [2, 5, 10],
    'model__min_samples_leaf': [1, 2, 4],
    'model__criterion': ['gini', 'entropy']
}

# Parameter grid untuk XGBClassifier
param_grid_xgbc = {
    'model__n_estimators': [100, 200, 300],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__max_depth': [3, 5, 7],
    'model__subsample': [0.6, 0.8, 1.0]
}

# Parameter grid untuk LGBMClassifier
param_grid_lgbmc = {
    'model__n_estimators': [100, 200, 300],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__max_depth': [3, 5, 7],
    'model__num_leaves': [20, 30, 40]
}

# Parameter grid untuk HistGradientBoostingClassifier
param_grid_hgbc = {
    'model__max_iter': [100, 200],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__max_depth': [3, 5, 7]
}

# Parameter grid untuk RandomForest
param_grid_rf = {
    'model__n_estimators': [100, 200, 300, 400, 500],  # Jumlah pohon dalam hutan
    'model__max_depth': [None, 10, 20, 30, 40, 50],  # Kedalaman maksimum pohon
    'model__min_samples_split': [2, 5, 10],  # Jumlah sampel minimum untuk membagi node
    'model__min_samples_leaf': [1, 2, 4],  # Jumlah sampel minimum per daun
    'model__bootstrap': [True, False]  # Jika menggunakan bootstrap sampling
}


# Pipeline untuk setiap model
pipeline_dtc = Pipeline([('model', DecisionTreeClassifier())])
pipeline_xgbc = Pipeline([('model', XGBClassifier())])
pipeline_lgbmc = Pipeline([('model', LGBMClassifier())])
pipeline_hgbc = Pipeline([('model', HistGradientBoostingClassifier())])
pipeline_rf = Pipeline([('model', RandomForestClassifier())])

# Randomized Search untuk masing-masing model
# Randomized Search untuk masing-masing model
random_search_dtc = RandomizedSearchCV(
    pipeline_dtc,
    param_distributions=param_grid_dtc,
    n_iter=20,  # Kurangi iterasi
    cv=3,       # Kurangi jumlah lipatan
    scoring='f1',  # Gunakan F1-score sebagai metrik evaluasi
    n_jobs=-1,
    random_state=42
)

random_search_xgbc = RandomizedSearchCV(
    pipeline_xgbc,
    param_distributions=param_grid_xgbc,
    n_iter=20,  # Kurangi iterasi
    cv=3,       # Kurangi jumlah lipatan
    scoring='f1',  # Gunakan F1-score sebagai metrik evaluasi
    n_jobs=-1,
    random_state=42
)

random_search_lgbmc = RandomizedSearchCV(
    pipeline_lgbmc,
    param_distributions=param_grid_lgbmc,
    n_iter=20,  # Kurangi iterasi
    cv=3,       # Kurangi jumlah lipatan
    scoring='f1',  # Gunakan F1-score sebagai metrik evaluasi
    n_jobs=-1,
    random_state=42
)

random_search_hgbc = RandomizedSearchCV(
    pipeline_hgbc,
    param_distributions=param_grid_hgbc,
    n_iter=20,  # Kurangi iterasi
    cv=3,       # Kurangi jumlah lipatan
    scoring='f1',  # Gunakan F1-score sebagai metrik evaluasi
    n_jobs=-1,
    random_state=42
)

random_search_rf = RandomizedSearchCV(
    pipeline_rf,
    param_distributions=param_grid_rf,
    n_iter=20,  # Kurangi iterasi
    cv=3,       # Kurangi jumlah lipatan
    scoring='f1',  # Gunakan F1-score sebagai metrik evaluasi
    n_jobs=-1,
    random_state=42
)

# Fit semua model
random_search_dtc.fit(X_train, y_train)
random_search_xgbc.fit(X_train, y_train)
random_search_lgbmc.fit(X_train, y_train)
random_search_hgbc.fit(X_train, y_train)
random_search_rf.fit(X_train, y_train)

# Best estimators
best_dtc = random_search_dtc.best_estimator_
best_xgbc = random_search_xgbc.best_estimator_
best_lgbmc = random_search_lgbmc.best_estimator_
best_hgbc = random_search_hgbc.best_estimator_
best_rf = random_search_rf.best_estimator_

# Print best parameters
print("Best parameters for DecisionTreeClassifier:", random_search_dtc.best_params_)
print("Best parameters for XGBClassifier:", random_search_xgbc.best_params_)
print("Best parameters for LGBMClassifier:", random_search_lgbmc.best_params_)
print("Best parameters for HistGradientBoostingClassifier:", random_search_hgbc.best_params_)
print("Best parameters for RandomForestClassifier:", random_search_rf.best_params_)

"""## Make prediction"""

# Predict
y_train_pred_dtc = best_dtc.predict(X_train)
y_test_pred_dtc = best_dtc.predict(X_test)

y_train_pred_xgbc = best_xgbc.predict(X_train)
y_test_pred_xgbc = best_xgbc.predict(X_test)

y_train_pred_lgbmc = best_lgbmc.predict(X_train)
y_test_pred_lgbmc = best_lgbmc.predict(X_test)

y_train_pred_hgbc = best_hgbc.predict(X_train)
y_test_pred_hgbc = best_hgbc.predict(X_test)

y_train_pred_rf = best_rf.predict(X_train)
y_test_pred_rf = best_rf.predict(X_test)

"""## Classification report

### Classification report data train
"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report


# Evaluation metrics for Decision Tree Classifier on training data
accuracy_train_dtc = accuracy_score(y_train, y_train_pred_dtc)
precision_train_dtc = precision_score(y_train, y_train_pred_dtc, average='micro')
recall_train_dtc = recall_score(y_train, y_train_pred_dtc, average='micro')
f1_train_dtc = f1_score(y_train, y_train_pred_dtc, average='micro')
cm_train_dtc = confusion_matrix(y_train, y_train_pred_dtc)

print(f'Decision Tree Classifier (Train) Accuracy: {accuracy_train_dtc:.2f}')
print(f'Decision Tree Classifier (Train) Precision: {precision_train_dtc:.2f}')
print(f'Decision Tree Classifier (Train) Recall: {recall_train_dtc:.2f}')
print(f'Decision Tree Classifier (Train) F1-Score: {f1_train_dtc:.2f}')
print(f'Decision Tree Classifier (Train) Confusion Matrix:\n{cm_train_dtc}')
print(f'Decision Tree Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_pred_dtc)}')

# Evaluation metrics for XGBoost Classifier on training data
accuracy_train_xgbc = accuracy_score(y_train, y_train_pred_xgbc)
precision_train_xgbc = precision_score(y_train, y_train_pred_xgbc, average='micro')
recall_train_xgbc = recall_score(y_train, y_train_pred_xgbc, average='micro')
f1_train_xgbc = f1_score(y_train, y_train_pred_xgbc, average='micro')
cm_train_xgbc = confusion_matrix(y_train, y_train_pred_xgbc)

print(f'XGBoost Classifier (Train) Accuracy: {accuracy_train_xgbc:.2f}')
print(f'XGBoost Classifier (Train) Precision: {precision_train_xgbc:.2f}')
print(f'XGBoost Classifier (Train) Recall: {recall_train_xgbc:.2f}')
print(f'XGBoost Classifier (Train) F1-Score: {f1_train_xgbc:.2f}')
print(f'XGBoost Classifier (Train) Confusion Matrix:\n{cm_train_xgbc}')
print(f'XGBoost Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_pred_xgbc)}')

# Evaluation metrics for LightGBM Classifier on training data
accuracy_train_lgbmc = accuracy_score(y_train, y_train_pred_lgbmc)
precision_train_lgbmc = precision_score(y_train, y_train_pred_lgbmc, average='micro')
recall_train_lgbmc = recall_score(y_train, y_train_pred_lgbmc, average='micro')
f1_train_lgbmc = f1_score(y_train, y_train_pred_lgbmc, average='micro')
cm_train_lgbmc = confusion_matrix(y_train, y_train_pred_lgbmc)

print(f'LightGBM Classifier (Train) Accuracy: {accuracy_train_lgbmc:.2f}')
print(f'LightGBM Classifier (Train) Precision: {precision_train_lgbmc:.2f}')
print(f'LightGBM Classifier (Train) Recall: {recall_train_lgbmc:.2f}')
print(f'LightGBM Classifier (Train) F1-Score: {f1_train_lgbmc:.2f}')
print(f'LightGBM Classifier (Train) Confusion Matrix:\n{cm_train_lgbmc}')
print(f'LightGBM Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_pred_lgbmc)}')

# Evaluation metrics for HistGradientBoosting Classifier on training data
accuracy_train_hgbc = accuracy_score(y_train, y_train_pred_hgbc)
precision_train_hgbc = precision_score(y_train, y_train_pred_hgbc, average='micro')
recall_train_hgbc = recall_score(y_train, y_train_pred_hgbc, average='micro')
f1_train_hgbc = f1_score(y_train, y_train_pred_hgbc, average='micro')
cm_train_hgbc = confusion_matrix(y_train, y_train_pred_hgbc)

print(f'HistGradientBoosting Classifier (Train) Accuracy: {accuracy_train_hgbc:.2f}')
print(f'HistGradientBoosting Classifier (Train) Precision: {precision_train_hgbc:.2f}')
print(f'HistGradientBoosting Classifier (Train) Recall: {recall_train_hgbc:.2f}')
print(f'HistGradientBoosting Classifier (Train) F1-Score: {f1_train_hgbc:.2f}')
print(f'HistGradientBoosting Classifier (Train) Confusion Matrix:\n{cm_train_hgbc}')
print(f'HistGradientBoosting Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_pred_hgbc)}')

# Evaluation metrics for RandomForest Classifier on training data
accuracy_train_rf = accuracy_score(y_train, y_train_pred_rf)
precision_train_rf = precision_score(y_train, y_train_pred_rf, average='micro')
recall_train_rf = recall_score(y_train, y_train_pred_rf, average='micro')
f1_train_rf = f1_score(y_train, y_train_pred_rf, average='micro')
cm_train_rf = confusion_matrix(y_train, y_train_pred_rf)

print(f'RandomForest Classifier (Train) Accuracy: {accuracy_train_rf:.2f}')
print(f'RandomForest Classifier (Train) Precision: {precision_train_rf:.2f}')
print(f'RandomForest Classifier (Train) Recall: {recall_train_rf:.2f}')
print(f'RandomForest Classifier (Train) F1-Score: {f1_train_rf:.2f}')
print(f'RandomForest Classifier (Train) Confusion Matrix:\n{cm_train_rf}')
print(f'RandomForest Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_pred_rf)}')

# Repeat the evaluation for y_test as you did before for each model

"""### Classification report for Data Test"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Evaluation metrics for Decision Tree Classifier
accuracy_dtc = accuracy_score(y_test, y_test_pred_dtc)
precision_dtc = precision_score(y_test, y_test_pred_dtc, average='micro')
recall_dtc = recall_score(y_test, y_test_pred_dtc, average='micro')
f1_dtc = f1_score(y_test, y_test_pred_dtc, average='micro')
cm_dtc = confusion_matrix(y_test, y_test_pred_dtc)

print(f'Decision Tree Classifier Accuracy: {accuracy_dtc:.2f}')
print(f'Decision Tree Classifier Precision: {precision_dtc:.2f}')
print(f'Decision Tree Classifier Recall: {recall_dtc:.2f}')
print(f'Decision Tree Classifier F1-Score: {f1_dtc:.2f}')
print(f'Decision Tree Classifier Confusion Matrix:\n{cm_dtc}')
print(f'Decision Tree Classifier Classification Report:\n{classification_report(y_test, y_test_pred_dtc)}')

# Evaluation metrics for XGBoost Classifier
accuracy_xgbc = accuracy_score(y_test, y_test_pred_xgbc)
precision_xgbc = precision_score(y_test, y_test_pred_xgbc, average='micro')
recall_xgbc = recall_score(y_test, y_test_pred_xgbc, average='micro')
f1_xgbc = f1_score(y_test, y_test_pred_xgbc, average='micro')
cm_xgbc = confusion_matrix(y_test, y_test_pred_xgbc)

print(f'XGBoost Classifier Accuracy: {accuracy_xgbc:.2f}')
print(f'XGBoost Classifier Precision: {precision_xgbc:.2f}')
print(f'XGBoost Classifier Recall: {recall_xgbc:.2f}')
print(f'XGBoost Classifier F1-Score: {f1_xgbc:.2f}')
print(f'XGBoost Classifier Confusion Matrix:\n{cm_xgbc}')
print(f'XGBoost Classifier Classification Report:\n{classification_report(y_test, y_test_pred_xgbc)}')

# Evaluation metrics for LightGBM Classifier
accuracy_lgbmc = accuracy_score(y_test, y_test_pred_lgbmc)
precision_lgbmc = precision_score(y_test, y_test_pred_lgbmc, average='micro')
recall_lgbmc = recall_score(y_test, y_test_pred_lgbmc, average='micro')
f1_lgbmc = f1_score(y_test, y_test_pred_lgbmc, average='micro')
cm_lgbmc = confusion_matrix(y_test, y_test_pred_lgbmc)

print(f'LightGBM Classifier Accuracy: {accuracy_lgbmc:.2f}')
print(f'LightGBM Classifier Precision: {precision_lgbmc:.2f}')
print(f'LightGBM Classifier Recall: {recall_lgbmc:.2f}')
print(f'LightGBM Classifier F1-Score: {f1_lgbmc:.2f}')
print(f'LightGBM Classifier Confusion Matrix:\n{cm_lgbmc}')
print(f'LightGBM Classifier Classification Report:\n{classification_report(y_test, y_test_pred_lgbmc)}')

# Evaluation metrics for HistGradientBoosting Classifier
accuracy_hgbc = accuracy_score(y_test, y_test_pred_hgbc)
precision_hgbc = precision_score(y_test, y_test_pred_hgbc, average='micro')
recall_hgbc = recall_score(y_test, y_test_pred_hgbc, average='micro')
f1_hgbc = f1_score(y_test, y_test_pred_hgbc, average='micro')
cm_hgbc = confusion_matrix(y_test, y_test_pred_hgbc)

print(f'HistGradientBoosting Classifier Accuracy: {accuracy_hgbc:.2f}')
print(f'HistGradientBoosting Classifier Precision: {precision_hgbc:.2f}')
print(f'HistGradientBoosting Classifier Recall: {recall_hgbc:.2f}')
print(f'HistGradientBoosting Classifier F1-Score: {f1_hgbc:.2f}')
print(f'HistGradientBoosting Classifier Confusion Matrix:\n{cm_hgbc}')
print(f'HistGradientBoosting Classifier Classification Report:\n{classification_report(y_test, y_test_pred_hgbc)}')

# Evaluation metrics for RandomForest Classifier
accuracy_rf = accuracy_score(y_test, y_test_pred_rf)
precision_rf = precision_score(y_test, y_test_pred_rf, average='micro')
recall_rf = recall_score(y_test, y_test_pred_rf, average='micro')
f1_rf = f1_score(y_test, y_test_pred_rf, average='micro')
cm_rf = confusion_matrix(y_test, y_test_pred_rf)

print(f'RandomForest Classifier Accuracy: {accuracy_rf:.2f}')
print(f'RandomForest Classifier Precision: {precision_rf:.2f}')
print(f'RandomForest Classifier Recall: {recall_rf:.2f}')
print(f'RandomForest Classifier F1-Score: {f1_rf:.2f}')
print(f'RandomForest Classifier Confusion Matrix:\n{cm_rf}')
print(f'RandomForest Classifier Classification Report:\n{classification_report(y_test, y_test_pred_rf)}')

"""### Combined to DataFrame Classification report"""

import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Create a dictionary to store the results
results = {}

# Decision Tree Classifier
results['Decision Tree'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_pred_dtc),
    'Precision (Train)': precision_score(y_train, y_train_pred_dtc, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_pred_dtc, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_pred_dtc, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_pred_dtc),
    'Precision (Test)': precision_score(y_test, y_test_pred_dtc, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_pred_dtc, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_pred_dtc, average='micro')
}

# XGBoost Classifier
results['XGBoost'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_pred_xgbc),
    'Precision (Train)': precision_score(y_train, y_train_pred_xgbc, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_pred_xgbc, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_pred_xgbc, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_pred_xgbc),
    'Precision (Test)': precision_score(y_test, y_test_pred_xgbc, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_pred_xgbc, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_pred_xgbc, average='micro')
}

# LightGBM Classifier
results['LightGBM'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_pred_lgbmc),
    'Precision (Train)': precision_score(y_train, y_train_pred_lgbmc, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_pred_lgbmc, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_pred_lgbmc, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_pred_lgbmc),
    'Precision (Test)': precision_score(y_test, y_test_pred_lgbmc, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_pred_lgbmc, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_pred_lgbmc, average='micro')
}

# HistGradientBoosting Classifier
results['HistGradientBoosting'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_pred_hgbc),
    'Precision (Train)': precision_score(y_train, y_train_pred_hgbc, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_pred_hgbc, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_pred_hgbc, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_pred_hgbc),
    'Precision (Test)': precision_score(y_test, y_test_pred_hgbc, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_pred_hgbc, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_pred_hgbc, average='micro')
}

# RandomForest Classifier
results['RandomForest'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_pred_rf),
    'Precision (Train)': precision_score(y_train, y_train_pred_rf, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_pred_rf, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_pred_rf, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_pred_rf),
    'Precision (Test)': precision_score(y_test, y_test_pred_rf, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_pred_rf, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_pred_rf, average='micro')
}

# Convert results to a DataFrame
df_results = pd.DataFrame(results).T
print(df_results)

df_results

import pandas as pd

# Data yang tersedia
data = {
    "Model": [
        "Decision Tree", "XGBoost", "LightGBM",
        "HistGradientBoosting", "RandomForest"
    ],
    "Train Accuracy": [0.89, 0.88, 0.89, 0.89, 0.89],
    "Train F1-Score (0)": [0.93, 0.93, 0.94, 0.94, 0.94],
    "Test Accuracy": [0.84, 0.86, 0.86, 0.86, 0.86],
    "Test F1-Score (0)": [0.90, 0.92, 0.92, 0.91, 0.92],
}

# Membuat DataFrame
df = pd.DataFrame(data)

# Menampilkan DataFrame
df

"""## Confusion Metrix"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Setup figure and axes
fig, axs = plt.subplots(1, 5, figsize=(25, 5))  # 1 baris, 5 kolom

# Visualisasi Confusion matrix pada model Decision Tree Classifier
cm_dtc = confusion_matrix(y_test, y_test_pred_dtc)
sns.heatmap(cm_dtc, annot=True, fmt='d', cmap='Blues', ax=axs[0])
axs[0].set_title('Decision Tree Classifier')
axs[0].set_xlabel('Predicted')
axs[0].set_ylabel('Actual')

# Visualisasi  Confusion matrix pada model XGBoost Classifier
cm_xgbc = confusion_matrix(y_test, y_test_pred_xgbc)
sns.heatmap(cm_xgbc, annot=True, fmt='d', cmap='Greens', ax=axs[1])
axs[1].set_title('XGBoost Classifier')
axs[1].set_xlabel('Predicted')
axs[1].set_ylabel('Actual')

# Visualisasi Confusion matrix pada model LightGBM Classifier
cm_lgbmc = confusion_matrix(y_test, y_test_pred_lgbmc)
sns.heatmap(cm_lgbmc, annot=True, fmt='d', cmap='Blues', ax=axs[2])
axs[2].set_title('LightGBM Classifier')
axs[2].set_xlabel('Predicted')
axs[2].set_ylabel('Actual')

# Visualisasi Confusion matrix pada model HistGradientBoosting Classifier
cm_hgbc = confusion_matrix(y_test, y_test_pred_hgbc)
sns.heatmap(cm_hgbc, annot=True, fmt='d', cmap='Oranges', ax=axs[3])
axs[3].set_title('HistGradientBoosting Classifier')
axs[3].set_xlabel('Predicted')
axs[3].set_ylabel('Actual')

# Visualisasi Confusion matrix pada model RandomForest Classifier
cm_rf = confusion_matrix(y_test, y_test_pred_rf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Reds', ax=axs[4])
axs[4].set_title('RandomForest Classifier')
axs[4].set_xlabel('Predicted')
axs[4].set_ylabel('Actual')

# Adjust layout
plt.tight_layout()
plt.show()

"""## ROC-AUC Without SMOTE"""

# Predict probabilities for ROC-AUC
y_train_pred_prob_dtc = best_dtc.predict_proba(X_train)[:, 1]
y_test_pred_prob_dtc = best_dtc.predict_proba(X_test)[:, 1]

y_train_pred_prob_xgbc = best_xgbc.predict_proba(X_train)[:, 1]
y_test_pred_prob_xgbc = best_xgbc.predict_proba(X_test)[:, 1]

y_train_pred_prob_lgbmc = best_lgbmc.predict_proba(X_train)[:, 1]
y_test_pred_prob_lgbmc = best_lgbmc.predict_proba(X_test)[:, 1]

y_train_pred_prob_hgbc = best_hgbc.predict_proba(X_train)[:, 1]
y_test_pred_prob_hgbc = best_hgbc.predict_proba(X_test)[:, 1]

y_train_pred_prob_rf = best_rf.predict_proba(X_train)[:, 1]
y_test_pred_prob_rf = best_rf.predict_proba(X_test)[:, 1]

from sklearn.metrics import roc_curve, auc

from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Store models and their probabilities for plotting
models_probs = {
    "Decision Tree": (y_test_pred_prob_dtc, "green"),
    "XGBoost": (y_test_pred_prob_xgbc, "blue"),
    "LightGBM": (y_test_pred_prob_lgbmc, "orange"),
    "HistGradientBoosting": (y_test_pred_prob_hgbc, "red"),
    "Random Forest": (y_test_pred_prob_rf, "purple"),
}

plt.figure(figsize=(12, 8))

for model_name, (probs, color) in models_probs.items():
    # Compute ROC curve and AUC
    fpr, tpr, thresholds = roc_curve(y_test, probs)
    roc_auc = auc(fpr, tpr)

    # Find optimal threshold (minimizing distance to (0, 1))
    optimal_idx = np.argmax(tpr - fpr)
    optimal_threshold = thresholds[optimal_idx]
    optimal_point = (fpr[optimal_idx], tpr[optimal_idx])

    # Plot ROC curve
    plt.plot(fpr, tpr, color=color, lw=2, label=f"{model_name} (AUC = {roc_auc:.2f})")
    plt.scatter(optimal_point[0], optimal_point[1], color=color, label=f"Optimal {model_name}", s=100)

    print(f"{model_name} - Optimal Threshold: {optimal_threshold:.2f}")

# Plot formatting
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate (FPR)", fontsize=14)
plt.ylabel("True Positive Rate (TPR)", fontsize=14)
plt.title("ROC Curves and Optimal Points for Churn Prediction Models", fontsize=16)
plt.legend(loc="lower right", fontsize=12)
plt.grid(alpha=0.3)
plt.show()

"""# Feature Importance WITHOUT SMOTE"""

!pip install dalex
!pip install scikit-plot
!pip install shap
!pip install eli5
!pip install lime

# import pandas for data wrangling
import pandas as pd
# import numpy for vectorize data manipulation
import numpy as np
# import matplotlib.pyplot module for data visualization
import matplotlib.pyplot as plt
# import seaborn for data visualization
import seaborn as sns
# import scipy for certain statistical function
from scipy import stats

# import train and test split method from scikit-learn
from sklearn.model_selection import train_test_split
# import metrics method for model evaluation
import sklearn.metrics as metrics
# import random forest classifier
from sklearn.ensemble import RandomForestClassifier
# import multi-layer perceptron
from sklearn.neural_network import MLPClassifier
# import decision tree model as surrogate model
from sklearn.tree import DecisionTreeClassifier
# import tree module
from sklearn import tree

# import xgboost classifier
from xgboost import XGBClassifier

# import dalex to explain complex model
import dalex as dx

# load LimeTabularExplainer for LIME method
from lime.lime_tabular import LimeTabularExplainer

cr = pd.read_csv('/content/Bank Customer Churn Prediction.csv')
cr

dummies_country = pd.get_dummies(cr['country'],prefix='et')

# tujuannya untuk menghindai multicolinearity
dummies_country.head()

cr_encoded = pd.concat([cr, dummies_country], axis=1)
cr_encoded = cr_encoded.drop('country', axis=1)
cr_encoded["et_France"] = cr_encoded["et_France"].astype(int)
cr_encoded["et_Germany"] = cr_encoded["et_Germany"].astype(int)
cr_encoded["et_Spain"] = cr_encoded["et_Spain"].astype(int)
cr_encoded.head()

from sklearn.preprocessing import LabelEncoder

# Mengubah variabel kategorikal menjadi numerik menggunakan Label Encoding
label_encoder = LabelEncoder()
for column in cr.columns:
    if cr[column].dtype == 'object':
        cr[column] = label_encoder.fit_transform(cr[column])

# Memeriksa data setelah encoding
cr.head()

"""## Train-Test Split Data"""

# cretate predictor variables as X
X = cr.drop(['churn'], axis = 1)
# create target data as y
y = cr['churn']

# split data to train and test data
# where 30% of churn_data are test data
# stratify data based on y varibales
# and freeze RNG with random_state = 1000
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.3,
    stratify = y,
    random_state=1000
)

"""## Modelling

**Random Forest Model**
```python
sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)
```
For further explanation, check: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
"""

random_forest_clf = RandomForestClassifier(
    random_state = 1000,
    n_estimators = 100
)

# fit model to training data
random_forest_clf.fit(X_train, y_train)

"""**Multi-Layered Perceptron**
```python
sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)
```
For further explanation, check: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html
"""

mlp_clf = MLPClassifier(
    random_state = 1000,
    hidden_layer_sizes = (8,),
    solver = 'sgd'
)

# fit model to training data
mlp_clf.fit(X_train, y_train)

"""**XGBoost Classifier**
```python
xgboost.XGBClassifier(*, objective='binary:logistic', use_label_encoder=True, **kwargs)
```
For further explanation, check: https://xgboost.readthedocs.io/en/stable/python/python_api.html
"""

# fit model no training data
xgb_clf = XGBClassifier(
    random_state = 1000
)

xgb_clf.fit(X_train, y_train)

"""### Model Evaluation"""

# random forest prediction
rf_pred = random_forest_clf.predict(X_test)
rf_pred_proba = random_forest_clf.predict_proba(X_test)

# multi-layered perceptron prediction
mlp_pred = mlp_clf.predict(X_test)
mlp_pred_proba = mlp_clf.predict_proba(X_test)

# XGBoost prediction
xgb_pred = xgb_clf.predict(X_test)
xgb_pred_proba = xgb_clf.predict_proba(X_test)

# XGBoost prediction
xgb_pred = xgb_clf.predict(X_test)
xgb_pred_proba = xgb_clf.predict_proba(X_test)

"""### **Classification Report**"""

# random forest prediction result
pd.DataFrame(metrics.classification_report(y_test, rf_pred, target_names=['0','1'], output_dict=True))

# multi-layered perceptron prediction result
pd.DataFrame(metrics.classification_report(y_test, mlp_pred, target_names=['0','1'], output_dict=True))

# XGBoost prediction result
pd.DataFrame(metrics.classification_report(y_test, xgb_pred, target_names=['0','1'], output_dict=True))

"""### Model Agnostic Report"""

# Inititate Explainer for all models

## initiate explainer for Random Forest model
churn_rf_exp = dx.Explainer(random_forest_clf, X_train, y_train, label = "RF Interpretation")
## initiate explainer for MLP model
churn_mlp_exp = dx.Explainer(mlp_clf, X_train, y_train, label = "MLP Interpretation")
## initiate explainer for XGBoost model
churn_xgb_exp = dx.Explainer(xgb_clf, X_train, y_train, label = "XGBoost Interpretation")

"""### Feature Importance"""

# visualizr permutation feature importance for Random Forest Model
churn_rf_exp.model_parts().plot()

# visualize permutation feature importance for MLP model
churn_mlp_exp.model_parts().plot()

# visualize permutation feature importance for XGBoost model
churn_xgb_exp.model_parts().plot()

"""## Partial Dependence Plot"""

# create partial dependence plot of Random Forest model
churn_rf_exp.model_profile().plot()

# create partial dependence plot of MLP model
churn_mlp_exp.model_profile().plot()

# create partial dependence plot of XGBoost model
churn_xgb_exp.model_profile().plot()

"""#### **Instance level**"""

!pip install shap

import shap
shap.initjs()

X_train.iloc[20,:]

y_train[1]

# Create ShapExplainer for XGBoost Model with all the rows of train data
xgb_explainer = shap.TreeExplainer(xgb_clf)
#Create ShapValues from XgbExplainer
xgb_shap_values = xgb_explainer.shap_values(X_train)

# initiate javascript module
shap.initjs()

#explain prediction for the first row of X_train
shap.force_plot(xgb_explainer.expected_value, xgb_shap_values[1,:], X_train.iloc[1,:])

"""#### **Interpretasi dan insight**

age, credit_score, active_number : memiliki dampak positif terhadap customer retain/tidak


"""

#Create ShapValues from XgbExplainer
xgb_shap_values = shap.TreeExplainer(xgb_clf).shap_values(X_train[:1000])

#Create Shap Summary Plot with next 1000 rows of train data
shap.summary_plot(xgb_shap_values, X_train[1000:2000],plot_type='bar')

import shap

# Tentukan Customer ID yang ingin dijelaskan
customer_id = 15571958  # Ganti dengan ID pelanggan yang diinginkan

# Pastikan customer_id adalah indeks di X_train
if 'customer_id' in X_train.columns:
    X_train.set_index('customer_id', inplace=True)

# Ambil data pelanggan berdasarkan customer_id
customer_data = X_train.loc[customer_id].values.reshape(1, -1)

# Pastikan customer_data memiliki kolom yang sesuai dengan X_train
customer_data = pd.DataFrame(customer_data, columns=X_train.columns)

# Gunakan SHAP untuk menjelaskan prediksi
explainer = shap.TreeExplainer(xgb_clf)  # Gunakan model xgboost
shap_values = explainer.shap_values(customer_data)

# Visualisasikan hasil SHAP menggunakan force plot untuk satu sampel
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], customer_data, feature_names=X_train.columns)

#Create SHAP summary plot to visualize impact distributions of next 1000 rows of train data
shap.summary_plot(xgb_shap_values, X_train[1000:2000])

"""### **Multiple single Predicition**"""

# initiate javascript module
shap.initjs()

#explain prediction for the first row of X_train
shap.force_plot(xgb_explainer.expected_value, xgb_shap_values[:1000,:],X_train.iloc[:1000,:])

"""#### **Interpretasi dan Insight**

Customer berdasarkan age, ternyata ada yg rentan retain karena berdampak negatif terhadap retain  di rentang usia 25-40 tahun

Namun, berdampak positif pada retain, terhadap rentang usia 40-64 tahun.

## Local Interpretable Model-Agnostic Explanation **(LIME)**
"""

#Define RandomForest Explainer with LIME module
lime_explainer = LimeTabularExplainer(X_train.values,
                                       feature_names=X_train.columns.tolist(),
                                       class_names=['churn','retained'],
                                       discretize_continuous=True,
                                       verbose = True
                                      )

data = X_train.loc[239]  # Menggunakan .loc untuk akses berdasarkan indeks
print(data)

customer_id_15606229 = X_train.iloc[239]
customer_id_15606229

#Explain RandomForest predicition for customer_id_15606229
lime_explainer.explain_instance(customer_id_15606229, random_forest_clf.predict_proba).show_in_notebook(show_table=True)

#Explain RandomForest predicition for customer_id_15606229
lime_explainer.explain_instance(customer_id_15606229, mlp_clf.predict_proba).show_in_notebook(show_table=True)

#Explain RandomForest predicition for customer_id_15606229
lime_explainer.explain_instance(customer_id_15606229, xgb_clf.predict_proba).show_in_notebook(show_table=True)

"""HAHAHAHAH"""

customer_id_15571958 = X_train.iloc[902]
customer_id_15571958

#Explain RandomForest predicition for customer_id_15571958
lime_explainer.explain_instance(customer_id_15571958, xgb_clf.predict_proba).show_in_notebook(show_table=True)

# Pastikan customer_id adalah indeks di X_train
if 'customer_id' in X_train.columns:
    X_train.set_index('customer_id', inplace=True)

# Cek apakah customer_id ada di X_train
customer_id = 15571958
if customer_id in X_train.index:
    # Temukan posisi baris berdasarkan customer_id
    row_index = X_train.index.get_loc(customer_id)
    print(f"Posisi customer_id {customer_id}: {row_index}")

    # Akses data dengan .iloc menggunakan posisi indeks
    customer_data = X_train.iloc[row_index]
    print("Data pelanggan:")
    print(customer_data)
else:
    print(f"Customer ID {customer_id} tidak ditemukan di X_train.")

"""## Bandingkan hasil sebelum di modelling dengan yang sudah di partial dependence"""

# 1. Distribusi Churn vs Non-Churn
plt.figure(figsize=(6, 4))
sns.countplot(x='churn', data=cr, palette='Set2')
plt.title('Distribusi Churn vs Non-Churn')
plt.xlabel('Churn')
plt.ylabel('Jumlah Pelanggan')
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, xgb_pred)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='grey', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, xgb_pred)
plt.plot(recall, precision, color='blue')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

# Misalnya y_test adalah label asli dan xgb_pred adalah prediksi dari model
f1 = f1_score(y_test, xgb_pred)

# Visualisasi F1 Score
plt.figure(figsize=(6, 4))
plt.bar(['F1 Score'], [f1], color='blue')
plt.ylim(0, 1)
plt.title('F1 Score of the Model')
plt.ylabel('F1 Score')
plt.text(0, f1/2, f'{f1:.2f}', ha='center', va='bottom', color='white', fontsize=14)
plt.show()

"""# Modelling Machine Learning With SMOTE"""

cr = pd.read_csv('/content/Bank Customer Churn Prediction.csv')
cr

dummies_country = pd.get_dummies(cr['country'],prefix='et')

# tujuannya untuk menghindari multicolinearity
dummies_country.head()

cr_encoded = pd.concat([cr, dummies_country], axis=1)
cr_encoded = cr_encoded.drop('country', axis=1)
cr_encoded["et_France"] = cr_encoded["et_France"].astype(int)
cr_encoded["et_Germany"] = cr_encoded["et_Germany"].astype(int)
cr_encoded["et_Spain"] = cr_encoded["et_Spain"].astype(int)
cr_encoded.head()

from sklearn.preprocessing import LabelEncoder

# Mengubah variabel kategorikal menjadi numerik menggunakan Label Encoding
label_encoder = LabelEncoder()
for column in cr.columns:
    if cr[column].dtype == 'object':
        cr[column] = label_encoder.fit_transform(cr[column])

# Memeriksa data setelah encoding
cr.head()

"""# **Train-Test Split**"""

# drop certain columns
cr = cr.drop(['customer_id'], axis = 1)

# cretate predictor variables as X
X = cr.drop(['churn'], axis = 1)
# create target data as y
y = cr['churn']

from imblearn.over_sampling import SMOTE

# Step 4: Apply SMOTE for oversampling the minority class
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

# Step 5: Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)

"""### Pipeline for initialize models"""

# Pipeline untuk DecisionTreeClassifier
pipeline_dtc_SMOTE = Pipeline([
    ('model', DecisionTreeClassifier(random_state=42))
])

# Pipeline untuk XGBoost Classifier
pipeline_xgbc_SMOTE = Pipeline([
    ('model', XGBClassifier(random_state=42))
])

# Pipeline untuk LightGBM Classifier
pipeline_lgbmc_SMOTE = Pipeline([
    ('model', LGBMClassifier(random_state=42))
])

# Pipeline untuk HistGradientBoostingClassifier
pipeline_hgbc_SMOTE = Pipeline([
    ('model', HistGradientBoostingClassifier(max_iter=100, learning_rate=0.1, max_depth=3))
])

# Pipeline untuk RandomForsetClassifier
pipeline_rf_SMOTE = Pipeline([
    ('model', RandomForestClassifier())
])

"""## Hyperparameter grids"""

# Parameter grid untuk DecisionTreeClassifier
param_grid_dtc_SMOTE = {
    'model__max_depth': [None, 10, 20, 30],
    'model__min_samples_split': [2, 5, 10],
    'model__min_samples_leaf': [1, 2, 4],
    'model__criterion': ['gini', 'entropy']
}

# Parameter grid untuk XGBClassifier
param_grid_xgbc_SMOTE = {
    'model__n_estimators': [100, 200, 300],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__max_depth': [3, 5, 7],
    'model__subsample': [0.6, 0.8, 1.0]
}

# Parameter grid untuk LGBMClassifier
param_grid_lgbmc_SMOTE = {
    'model__n_estimators': [100, 200, 300],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__max_depth': [3, 5, 7],
    'model__num_leaves': [20, 30, 40]
}

# Parameter grid untuk HistGradientBoostingClassifier
param_grid_hgbc_SMOTE = {
    'model__max_iter': [100, 200],
    'model__learning_rate': [0.01, 0.1, 0.2],
    'model__max_depth': [3, 5, 7]
}

# Parameter grid untuk RandomForest
param_grid_rf_SMOTE = {
    'model__n_estimators': [100, 200, 300, 400, 500],  # Jumlah pohon dalam hutan
    'model__max_depth': [None, 10, 20, 30, 40, 50],  # Kedalaman maksimum pohon
    'model__min_samples_split': [2, 5, 10],  # Jumlah sampel minimum untuk membagi node
    'model__min_samples_leaf': [1, 2, 4],  # Jumlah sampel minimum per daun
    'model__bootstrap': [True, False]  # Jika menggunakan bootstrap sampling
}


# Pipeline untuk setiap model
pipeline_dtc_SMOTE = Pipeline([('model', DecisionTreeClassifier())])
pipeline_xgbc_SMOTE = Pipeline([('model', XGBClassifier())])
pipeline_lgbmc_SMOTE = Pipeline([('model', LGBMClassifier())])
pipeline_hgbc_SMOTE = Pipeline([('model', HistGradientBoostingClassifier())])
pipeline_rf_SMOTE = Pipeline([('model', RandomForestClassifier())])

# Randomized Search untuk masing-masing model
random_search_dtc_SMOTE = RandomizedSearchCV(
    pipeline_dtc_SMOTE,
    param_distributions=param_grid_dtc_SMOTE,
    n_iter=50,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

random_search_xgbc_SMOTE = RandomizedSearchCV(
    pipeline_xgbc_SMOTE,
    param_distributions=param_grid_xgbc_SMOTE,
    n_iter=50,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

random_search_lgbmc_SMOTE = RandomizedSearchCV(
    pipeline_lgbmc_SMOTE,
    param_distributions=param_grid_lgbmc_SMOTE,
    n_iter=50,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

random_search_hgbc_SMOTE = RandomizedSearchCV(
    pipeline_hgbc_SMOTE,
    param_distributions=param_grid_hgbc_SMOTE,
    n_iter=50,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

random_search_rf_SMOTE = RandomizedSearchCV(
    pipeline_rf_SMOTE,
    param_distributions=param_grid_rf_SMOTE,
    n_iter=50,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

# Fit semua model
random_search_dtc_SMOTE.fit(X_train, y_train)
random_search_xgbc_SMOTE.fit(X_train, y_train)
random_search_lgbmc_SMOTE.fit(X_train, y_train)
random_search_hgbc_SMOTE.fit(X_train, y_train)
random_search_rf_SMOTE.fit(X_train, y_train)

# Best estimators
best_dtc_SMOTE = random_search_dtc.best_estimator_
best_xgbc_SMOTE= random_search_xgbc.best_estimator_
best_lgbmc_SMOTE = random_search_lgbmc.best_estimator_
best_hgbc_SMOTE = random_search_hgbc.best_estimator_
best_rf_SMOTE = random_search_rf.best_estimator_

# Print best parameters
print("Best parameters for DecisionTreeClassifier:", random_search_dtc_SMOTE.best_params_)
print("Best parameters for XGBClassifier:", random_search_xgbc_SMOTE.best_params_)
print("Best parameters for LGBMClassifier:", random_search_lgbmc_SMOTE.best_params_)
print("Best parameters for HistGradientBoostingClassifier:", random_search_hgbc_SMOTE.best_params_)
print("Best parameters for RandomForestClassifier:", random_search_rf_SMOTE.best_params_)

"""## Make prediction"""

# Predict
y_train_SMOTE_pred_dtc = best_dtc_SMOTE.predict(X_train)
y_test_SMOTE_pred_dtc = best_dtc_SMOTE.predict(X_test)

y_train_SMOTE_pred_xgbc = best_xgbc_SMOTE.predict(X_train)
y_test_SMOTE_pred_xgbc = best_xgbc_SMOTE.predict(X_test)

y_train_SMOTE_pred_lgbmc = best_lgbmc_SMOTE.predict(X_train)
y_test_SMOTE_pred_lgbmc = best_lgbmc_SMOTE.predict(X_test)

y_train_SMOTE_pred_hgbc = best_hgbc_SMOTE.predict(X_train)
y_test_SMOTE_pred_hgbc = best_hgbc_SMOTE.predict(X_test)

y_train_SMOTE_pred_rf = best_rf_SMOTE.predict(X_train)
y_test_SMOTE_pred_rf = best_rf_SMOTE.predict(X_test)

"""## Classification report

### Classification report data train
"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report


# Evaluation metrics for Decision Tree Classifier on training data
accuracy_train_dtc = accuracy_score(y_train, y_train_SMOTE_pred_dtc)
precision_train_dtc = precision_score(y_train, y_train_SMOTE_pred_dtc, average='micro')
recall_train_dtc = recall_score(y_train, y_train_SMOTE_pred_dtc, average='micro')
f1_train_dtc = f1_score(y_train, y_train_SMOTE_pred_dtc, average='micro')
cm_train_dtc = confusion_matrix(y_train, y_train_SMOTE_pred_dtc)

print(f'Decision Tree Classifier (Train) Accuracy: {accuracy_train_dtc:.2f}')
print(f'Decision Tree Classifier (Train) Precision: {precision_train_dtc:.2f}')
print(f'Decision Tree Classifier (Train) Recall: {recall_train_dtc:.2f}')
print(f'Decision Tree Classifier (Train) F1-Score: {f1_train_dtc:.2f}')
print(f'Decision Tree Classifier (Train) Confusion Matrix:\n{cm_train_dtc}')
print(f'Decision Tree Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_SMOTE_pred_dtc)}')

# Evaluation metrics for XGBoost Classifier on training data
accuracy_train_xgbc = accuracy_score(y_train, y_train_SMOTE_pred_xgbc)
precision_train_xgbc = precision_score(y_train, y_train_SMOTE_pred_xgbc, average='micro')
recall_train_xgbc = recall_score(y_train, y_train_SMOTE_pred_xgbc, average='micro')
f1_train_xgbc = f1_score(y_train,y_train_SMOTE_pred_xgbc, average='micro')
cm_train_xgbc = confusion_matrix(y_train,y_train_SMOTE_pred_xgbc)

print(f'XGBoost Classifier (Train) Accuracy: {accuracy_train_xgbc:.2f}')
print(f'XGBoost Classifier (Train) Precision: {precision_train_xgbc:.2f}')
print(f'XGBoost Classifier (Train) Recall: {recall_train_xgbc:.2f}')
print(f'XGBoost Classifier (Train) F1-Score: {f1_train_xgbc:.2f}')
print(f'XGBoost Classifier (Train) Confusion Matrix:\n{cm_train_xgbc}')
print(f'XGBoost Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_SMOTE_pred_xgbc)}')

# Evaluation metrics for LightGBM Classifier on training data
accuracy_train_lgbmc = accuracy_score(y_train, y_train_SMOTE_pred_lgbmc)
precision_train_lgbmc = precision_score(y_train, y_train_SMOTE_pred_lgbmc, average='micro')
recall_train_lgbmc = recall_score(y_train, y_train_SMOTE_pred_lgbmc, average='micro')
f1_train_lgbmc = f1_score(y_train, y_train_SMOTE_pred_lgbmc, average='micro')
cm_train_lgbmc = confusion_matrix(y_train, y_train_SMOTE_pred_lgbmc)

print(f'LightGBM Classifier (Train) Accuracy: {accuracy_train_lgbmc:.2f}')
print(f'LightGBM Classifier (Train) Precision: {precision_train_lgbmc:.2f}')
print(f'LightGBM Classifier (Train) Recall: {recall_train_lgbmc:.2f}')
print(f'LightGBM Classifier (Train) F1-Score: {f1_train_lgbmc:.2f}')
print(f'LightGBM Classifier (Train) Confusion Matrix:\n{cm_train_lgbmc}')
print(f'LightGBM Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_SMOTE_pred_lgbmc)}')

# Evaluation metrics for HistGradientBoosting Classifier on training data
accuracy_train_hgbc = accuracy_score(y_train, y_train_SMOTE_pred_hgbc)
precision_train_hgbc = precision_score(y_train, y_train_SMOTE_pred_hgbc, average='micro')
recall_train_hgbc = recall_score(y_train, y_train_SMOTE_pred_hgbc, average='micro')
f1_train_hgbc = f1_score(y_train, y_train_SMOTE_pred_hgbc, average='micro')
cm_train_hgbc = confusion_matrix(y_train, y_train_SMOTE_pred_hgbc)

print(f'HistGradientBoosting Classifier (Train) Accuracy: {accuracy_train_hgbc:.2f}')
print(f'HistGradientBoosting Classifier (Train) Precision: {precision_train_hgbc:.2f}')
print(f'HistGradientBoosting Classifier (Train) Recall: {recall_train_hgbc:.2f}')
print(f'HistGradientBoosting Classifier (Train) F1-Score: {f1_train_hgbc:.2f}')
print(f'HistGradientBoosting Classifier (Train) Confusion Matrix:\n{cm_train_hgbc}')
print(f'HistGradientBoosting Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_SMOTE_pred_hgbc)}')

# Evaluation metrics for RandomForest Classifier on training data
accuracy_train_rf = accuracy_score(y_train, y_train_SMOTE_pred_rf)
precision_train_rf = precision_score(y_train, y_train_SMOTE_pred_rf, average='micro')
recall_train_rf = recall_score(y_train, y_train_SMOTE_pred_rf, average='micro')
f1_train_rf = f1_score(y_train, y_train_SMOTE_pred_rf, average='micro')
cm_train_rf = confusion_matrix(y_train, y_train_SMOTE_pred_rf)

print(f'RandomForest Classifier (Train) Accuracy: {accuracy_train_rf:.2f}')
print(f'RandomForest Classifier (Train) Precision: {precision_train_rf:.2f}')
print(f'RandomForest Classifier (Train) Recall: {recall_train_rf:.2f}')
print(f'RandomForest Classifier (Train) F1-Score: {f1_train_rf:.2f}')
print(f'RandomForest Classifier (Train) Confusion Matrix:\n{cm_train_rf}')
print(f'RandomForest Classifier (Train) Classification Report:\n{classification_report(y_train, y_train_SMOTE_pred_rf)}')

# Repeat the evaluation for y_test as you did before for each model

"""### Classification report for Data Test"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Evaluation metrics for Decision Tree Classifier
accuracy_dtc = accuracy_score(y_test, y_test_SMOTE_pred_dtc)
precision_dtc = precision_score(y_test, y_test_SMOTE_pred_dtc, average='micro')
recall_dtc = recall_score(y_test, y_test_SMOTE_pred_dtc, average='micro')
f1_dtc = f1_score(y_test, y_test_SMOTE_pred_dtc, average='micro')
cm_dtc = confusion_matrix(y_test, y_test_SMOTE_pred_dtc)

print(f'Decision Tree Classifier Accuracy: {accuracy_dtc:.2f}')
print(f'Decision Tree Classifier Precision: {precision_dtc:.2f}')
print(f'Decision Tree Classifier Recall: {recall_dtc:.2f}')
print(f'Decision Tree Classifier F1-Score: {f1_dtc:.2f}')
print(f'Decision Tree Classifier Confusion Matrix:\n{cm_dtc}')
print(f'Decision Tree Classifier Classification Report:\n{classification_report(y_test, y_test_SMOTE_pred_dtc)}')

# Evaluation metrics for XGBoost Classifier
accuracy_xgbc = accuracy_score(y_test, y_test_SMOTE_pred_xgbc)
precision_xgbc = precision_score(y_test, y_test_SMOTE_pred_xgbc, average='micro')
recall_xgbc = recall_score(y_test, y_test_SMOTE_pred_xgbc, average='micro')
f1_xgbc = f1_score(y_test, y_test_SMOTE_pred_xgbc, average='micro')
cm_xgbc = confusion_matrix(y_test, y_test_SMOTE_pred_xgbc)

print(f'XGBoost Classifier Accuracy: {accuracy_xgbc:.2f}')
print(f'XGBoost Classifier Precision: {precision_xgbc:.2f}')
print(f'XGBoost Classifier Recall: {recall_xgbc:.2f}')
print(f'XGBoost Classifier F1-Score: {f1_xgbc:.2f}')
print(f'XGBoost Classifier Confusion Matrix:\n{cm_xgbc}')
print(f'XGBoost Classifier Classification Report:\n{classification_report(y_test, y_test_SMOTE_pred_xgbc)}')

# Evaluation metrics for LightGBM Classifier
accuracy_lgbmc = accuracy_score(y_test, y_test_SMOTE_pred_lgbmc)
precision_lgbmc = precision_score(y_test, y_test_SMOTE_pred_lgbmc, average='micro')
recall_lgbmc = recall_score(y_test, y_test_SMOTE_pred_lgbmc, average='micro')
f1_lgbmc = f1_score(y_test, y_test_SMOTE_pred_lgbmc, average='micro')
cm_lgbmc = confusion_matrix(y_test, y_test_SMOTE_pred_lgbmc)

print(f'LightGBM Classifier Accuracy: {accuracy_lgbmc:.2f}')
print(f'LightGBM Classifier Precision: {precision_lgbmc:.2f}')
print(f'LightGBM Classifier Recall: {recall_lgbmc:.2f}')
print(f'LightGBM Classifier F1-Score: {f1_lgbmc:.2f}')
print(f'LightGBM Classifier Confusion Matrix:\n{cm_lgbmc}')
print(f'LightGBM Classifier Classification Report:\n{classification_report(y_test, y_test_SMOTE_pred_lgbmc)}')

# Evaluation metrics for HistGradientBoosting Classifier
accuracy_hgbc = accuracy_score(y_test, y_test_SMOTE_pred_hgbc)
precision_hgbc = precision_score(y_test, y_test_SMOTE_pred_hgbc, average='micro')
recall_hgbc = recall_score(y_test, y_test_SMOTE_pred_hgbc, average='micro')
f1_hgbc = f1_score(y_test, y_test_SMOTE_pred_hgbc, average='micro')
cm_hgbc = confusion_matrix(y_test, y_test_SMOTE_pred_hgbc)

print(f'HistGradientBoosting Classifier Accuracy: {accuracy_hgbc:.2f}')
print(f'HistGradientBoosting Classifier Precision: {precision_hgbc:.2f}')
print(f'HistGradientBoosting Classifier Recall: {recall_hgbc:.2f}')
print(f'HistGradientBoosting Classifier F1-Score: {f1_hgbc:.2f}')
print(f'HistGradientBoosting Classifier Confusion Matrix:\n{cm_hgbc}')
print(f'HistGradientBoosting Classifier Classification Report:\n{classification_report(y_test, y_test_SMOTE_pred_hgbc)}')

# Evaluation metrics for RandomForest Classifier
accuracy_rf = accuracy_score(y_test, y_test_SMOTE_pred_rf)
precision_rf = precision_score(y_test, y_test_SMOTE_pred_rf, average='micro')
recall_rf = recall_score(y_test, y_test_SMOTE_pred_rf, average='micro')
f1_rf = f1_score(y_test, y_test_SMOTE_pred_rf, average='micro')
cm_rf = confusion_matrix(y_test, y_test_SMOTE_pred_rf)

print(f'RandomForest Classifier Accuracy: {accuracy_rf:.2f}')
print(f'RandomForest Classifier Precision: {precision_rf:.2f}')
print(f'RandomForest Classifier Recall: {recall_rf:.2f}')
print(f'RandomForest Classifier F1-Score: {f1_rf:.2f}')
print(f'RandomForest Classifier Confusion Matrix:\n{cm_rf}')
print(f'RandomForest Classifier Classification Report:\n{classification_report(y_test, y_test_SMOTE_pred_rf)}')

"""### Combined to DataFrame Classification report"""

# Predict
y_train_SMOTE_pred_dtc = best_dtc.predict(X_train)
y_test_SMOTE_pred_dtc = best_dtc.predict(X_test)

y_train_SMOTE_pred_xgbc = best_xgbc.predict(X_train)
y_test_SMOTE_pred_xgbc = best_xgbc.predict(X_test)

y_train_SMOTE_pred_lgbmc = best_lgbmc.predict(X_train)
y_test_SMOTE_pred_lgbmc = best_lgbmc.predict(X_test)

y_train_SMOTE_pred_hgbc = best_hgbc.predict(X_train)
y_test_SMOTE_pred_hgbc = best_hgbc.predict(X_test)

y_train_SMOTE_pred_rf = best_rf.predict(X_train)
y_test_SMOTE_pred_rf = best_rf.predict(X_test)

import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Create a dictionary to store the results
results = {}

# Decision Tree Classifier
results['Decision Tree'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_SMOTE_pred_dtc),
    'Precision (Train)': precision_score(y_train, y_train_SMOTE_pred_dtc, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_SMOTE_pred_dtc, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_SMOTE_pred_dtc, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_SMOTE_pred_dtc),
    'Precision (Test)': precision_score(y_test, y_test_SMOTE_pred_dtc, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_SMOTE_pred_dtc, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_SMOTE_pred_dtc, average='micro')
}

# XGBoost Classifier
results['XGBoost'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_SMOTE_pred_xgbc),
    'Precision (Train)': precision_score(y_train, y_train_SMOTE_pred_xgbc, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_SMOTE_pred_xgbc, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_SMOTE_pred_xgbc, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_SMOTE_pred_xgbc),
    'Precision (Test)': precision_score(y_test, y_test_SMOTE_pred_xgbc, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_SMOTE_pred_xgbc, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_SMOTE_pred_xgbc, average='micro')
}

# LightGBM Classifier
results['LightGBM'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_SMOTE_pred_lgbmc),
    'Precision (Train)': precision_score(y_train, y_train_SMOTE_pred_lgbmc, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_SMOTE_pred_lgbmc, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_SMOTE_pred_lgbmc, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_SMOTE_pred_lgbmc),
    'Precision (Test)': precision_score(y_test, y_test_SMOTE_pred_lgbmc, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_SMOTE_pred_lgbmc, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_SMOTE_pred_lgbmc, average='micro')
}

# HistGradientBoosting Classifier
results['HistGradientBoosting'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_SMOTE_pred_hgbc),
    'Precision (Train)': precision_score(y_train, y_train_SMOTE_pred_hgbc, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_SMOTE_pred_hgbc, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_SMOTE_pred_hgbc, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_SMOTE_pred_hgbc),
    'Precision (Test)': precision_score(y_test, y_test_SMOTE_pred_hgbc, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_SMOTE_pred_hgbc, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_SMOTE_pred_hgbc, average='micro')
}

# RandomForest Classifier
results['RandomForest'] = {
    'Accuracy (Train)': accuracy_score(y_train, y_train_SMOTE_pred_rf),
    'Precision (Train)': precision_score(y_train, y_train_SMOTE_pred_rf, average='micro'),
    'Recall (Train)': recall_score(y_train, y_train_SMOTE_pred_rf, average='micro'),
    'F1-Score (Train)': f1_score(y_train, y_train_SMOTE_pred_rf, average='micro'),
    'Accuracy (Test)': accuracy_score(y_test, y_test_SMOTE_pred_rf),
    'Precision (Test)': precision_score(y_test, y_test_SMOTE_pred_rf, average='micro'),
    'Recall (Test)': recall_score(y_test, y_test_SMOTE_pred_rf, average='micro'),
    'F1-Score (Test)': f1_score(y_test, y_test_SMOTE_pred_rf, average='micro')
}

# Convert results to a DataFrame
df_results_SMOTE = pd.DataFrame(results).T
print(df_results_SMOTE)

df_results_SMOTE

import pandas as pd

# Data for the table
data = {
    "Model": [
        "Decision Tree Classifier",
        "XGBoost Classifier",
        "LightGBM Classifier",
        "HistGradientBoosting Classifier",
        "RandomForest Classifier",
    ],
    "Train Accuracy": [0.87, 1.00, 1.00, 0.92, 1.00],
    "Train F1-Score (0)": [0.87, 1.00, 1.00, 0.92, 1.00],
    "Test Accuracy": [0.81, 0.87, 0.87, 0.86, 0.86],
    "Test F1-Score (0)": [0.81, 0.87, 0.87, 0.86, 0.86],
}

# Create a dataframe
df_smote = pd.DataFrame(data)

df_smote

"""## Confusion Metrix"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Setup figure and axes
fig, axs = plt.subplots(1, 5, figsize=(25, 5))  # 1 baris, 5 kolom

# Visualisasi Confusion matrix pada model Decision Tree Classifier
cm_dtc_SMOTE = confusion_matrix(y_test, y_test_pred_dtc)
sns.heatmap(cm_dtc_SMOTE, annot=True, fmt='d', cmap='Blues', ax=axs[0])
axs[0].set_title('Decision Tree Classifier')
axs[0].set_xlabel('Predicted')
axs[0].set_ylabel('Actual')

# Visualisasi  Confusion matrix pada model XGBoost Classifier
cm_xgbc_SMOTE = confusion_matrix(y_test, y_test_SMOTE_pred_xgbc)
sns.heatmap(cm_xgbc_SMOTE, annot=True, fmt='d', cmap='Greens', ax=axs[1])
axs[1].set_title('XGBoost Classifier')
axs[1].set_xlabel('Predicted')
axs[1].set_ylabel('Actual')

# Visualisasi Confusion matrix pada model LightGBM Classifier
cm_lgbmc_SMOTE = confusion_matrix(y_test, y_test_SMOTE_pred_lgbmc)
sns.heatmap(cm_lgbmc_SMOTE, annot=True, fmt='d', cmap='Blues', ax=axs[2])
axs[2].set_title('LightGBM Classifier')
axs[2].set_xlabel('Predicted')
axs[2].set_ylabel('Actual')

# Visualisasi Confusion matrix pada model HistGradientBoosting Classifier
cm_hgbc_SMOTE = confusion_matrix(y_test, y_test_SMOTE_pred_hgbc)
sns.heatmap(cm_hgbc_SMOTE, annot=True, fmt='d', cmap='Oranges', ax=axs[3])
axs[3].set_title('HistGradientBoosting Classifier')
axs[3].set_xlabel('Predicted')
axs[3].set_ylabel('Actual')

# Visualisasi Confusion matrix pada model RandomForest Classifier
cm_rf_SMOTE = confusion_matrix(y_test, y_test_SMOTE_pred_rf)
sns.heatmap(cm_rf_SMOTE, annot=True, fmt='d', cmap='Reds', ax=axs[4])
axs[4].set_title('RandomForest Classifier')
axs[4].set_xlabel('Predicted')
axs[4].set_ylabel('Actual')

# Adjust layout
plt.tight_layout()
plt.show()

"""#### Interpretasi dan Insight Recommendation

**Kesimpulan dan Rekomendasi:**
*   **Pilih Model:** LightGBM atau XGBoost memiliki performa    
    terbaik dengan FN rendah, cocok untuk mencegah churn.

*   **Tindakan Bisnis:**
    Implementasikan program retention berdasarkan prediksi churn, misalnya menawarkan insentif (diskon atau penawaran khusus).
    
    Evaluasi lebih lanjut customer retained yang salah diprediksi (FP), untuk memastikan bahwa program churn tidak memboroskan sumber daya pada customer yang sebenarnya tetap loyal.

*   **Analisis Lanjutan:**
    
    Identifikasi pola pada data feature yang memengaruhi kesalahan prediksi untuk meningkatkan akurasi model lebih lanjut.
    Kombinasikan model (ensemble) jika memungkinkan untuk memaksimalkan prediksi.

## ROC-AUC
"""

from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Function to plot ROC curve
def plot_roc_curve(fpr, tpr, roc_auc, model_name):
    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (area = {roc_auc:.2f})')

# Evaluate ROC-AUC for each model
models = {
    'Decision Tree': best_dtc_SMOTE,
    'XGBoost': best_xgbc_SMOTE,
    'LightGBM': best_lgbmc_SMOTE,
    'HistGradientBoosting': best_hgbc_SMOTE,
    'Random Forest': best_rf_SMOTE
}

plt.figure(figsize=(10, 8))

# Loop through each model and calculate ROC-AUC
for model_name, model in models.items():
    # Predict probabilities for the test data
    y_prob = model.predict_proba(X_test)[:, 1]  # Get the probability of the positive class

    # Calculate ROC curve values
    fpr, tpr, _ = roc_curve(y_test, y_prob)

    # Calculate ROC-AUC score
    roc_auc = roc_auc_score(y_test, y_prob)

    # Plot ROC curve
    plot_roc_curve(fpr, tpr, roc_auc, model_name)

# Plot diagonal line (no-skill classifier)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')

# Customize plot
plt.title('Receiver Operating Characteristic (ROC) Curve for Different Models')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()